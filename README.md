# Local_LLM
Running local LLMs with Ollama to perform RAG for answering questions based on sample PDFs

- https://alekh-sinha09.medium.com/generative-ai-qa-model-using-chroma-and-mistral-7b-565088031e80