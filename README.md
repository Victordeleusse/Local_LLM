# Local_LLM
Running local LLMs with Ollama to perform RAG for answering questions based on sample PDFs
